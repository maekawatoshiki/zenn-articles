---
title: "ONNX について"
emoji: "🪨"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ONNX"]
published: true
---

# はじめに

> **ONNX is an open format built to represent machine learning models.**
>
> ONNX defines a common set of operators -
> the building blocks of machine learning and deep learning models -
> and a common file format to enable AI developers to use models with
> a variety of frameworks, tools, runtimes, and compilers.
>
> *[onnx.ai](https://onnx.ai/) より*

[ONNX](https://github.com/onnx/onnx) とは、機械学習モデルを表現するためのオープンなフォーマット（や周辺のエコシステム）を指します。

この記事では、あまり日本語の資料が見つからない部分、特に **ONNX フォーマットの構造や上手な扱い方**について触れていきます。

なお、必ずしも正確な情報を提供できるわけではないため、間違いなどはコメントしていただけると助かります。

# 便利なリンク集

- https://onnx.ai
    - 公式サイト
    - 各種ドキュメントへ飛ぶためのポータルサイトとしても機能している
- https://github.com/onnx/onnx
    - 公式 GitHub リポジトリ
    - フォーマットの定義や周辺ライブラリ・ツールなどを管理している
- https://github.com/onnx/onnx/blob/main/docs/
    - フォーマットのドキュメント
    - 困ったらこのディレクトリ内を探す
- https://github.com/onnx/onnx/blob/main/docs/IR.md
    - IR (中間表現）のドキュメント
- https://github.com/onnx/onnx/blob/main/docs/Operators.md
    - オペレータのドキュメント（一覧）
- https://github.com/onnx/models
    - ONNX 形式の学習済みモデルが多数置いてあるリポジトリ
    - PyTorch などからエクスポートする他にこういった場所からダウンロードすることもできる
- https://github.com/PINTO0309/PINTO_model_zoo
    - 個人で膨大な数の ONNX 形式モデルを管理されている方のリポジトリ
- https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md
    - Python 向け API の使い方

# ONNX の構造

ONNX フォーマット（以下 ONNX）は、[IR.md](https://github.com/onnx/onnx/blob/main/docs/IR.md) にもあるように、[Protocol Buffers として定義](https://github.com/onnx/onnx/blob/main/onnx/onnx.proto)されています。
つまり、`*.onnx` というファイルを見かけたら、Protocol Buffers としてデシリアライズすれば中身を覗くことができるというわけです。

まずは大まかな構造を把握して、その後に細かい部分を見ていきましょう。

## 全体像

ONNX は以下のような階層構造から成り立っています（いくつか省いている要素があります）。

- Model ([`onnx.ModelProto`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L348)) - モデル（ファイルのトップレベル要素）
    - Graph ([`onnx.GraphProto`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L447)) - 計算グラフ
        - Node ([`onnx.NodeProto`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L204)) - 計算ノード

また、これらの要素に含まれるフィールドとして以下が存在します（個人的に重要だと判断したものを抜粋しています）。

- Tensor ([`onnx.TensorProto`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L487)) - テンソル（重み）のデータ
- ValueInfo ([`onnx.ValueInfoProto`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L187)) - 値を説明する情報
- Attribute ([`onnx.AttributeProto`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L120)) - Node に指定される情報

## 解釈

ONNX は基本的に、なにか入力値を受け取って、なにか出力値を返すという計算グラフを表現するためのものです。

ここで、**入力値**（の配列）は **`model.graph.input`** (つまり [`ModelProto model > GraphProto graph > ValueInfoProto[] input`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L467)) に対応します（これはモデルの実行時に渡される値）。
同様に、**出力値**（の配列）は **`model.graph.output`** に対応します。
また、重みなどの**定数値**はしばしば **`model.graph.initializer`** として定義されており、これも入力値とみなせます（これは実行前から用意されている値）。

これらの入力値を **`model.graph.node`** (つまり [`ModelProto model > GraphProto graph > NodeProto[] node`](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/onnx/onnx-ml.proto#L449)) で指定された計算に従って処理することで、出力値を得ます。

## Operator

[Node](https://github.com/onnx/onnx/blob/a66b596981156097da5988cf6f9527817e5bcad7/docs/IR.md#nodes) には `op_type`（文字列）というフィールドがあり、ここに [Operators](https://github.com/onnx/onnx/blob/main/docs/Operators.md) にて紹介されている命令を指定することで、計算の種類を指定することができます。

**ある operator はある domain に属する**という定義となっていて、例えば [Operators](https://github.com/onnx/onnx/blob/main/docs/Operators.md) で定義されている operator はほとんどが `ai.onnx` というドメインに属しています。他には `com.microsoft.experimental` などのドメインが存在しています。

自ら新たなドメインを追加・そのドメインに属する operator を追加することで、ONNX の仕様を超えた計算を可能にすることもできます。

## Operator versioning

TODO...

# ONNX の扱い方

実際に ONNX ファイルを用いて開発を行う上で、便利な豆知識をまとめておきます。

## モデルの可視化

便利なコマンドとして [`netron`](https://github.com/lutzroeder/netron) があります。
ブラウザからモデルを確認することができ便利です。

```bash
# netron /path/to/model.onnx
Serving '/path/to/model.onnx' at http://localhost:8080
```

![netron](/images/netron.png)

## 便利なスニペット

### モデルの読み込み

PyPI からインストールできる `onnx` ライブラリを使うことで、Protocol Buffers をあまり意識せずに、モデルを扱うことができます。

```python
import onnx # pip install onnx
model = onnx.load("/path/to/model.onnx") # ModelProto として読み込まれる
```

### モデルの書き込み

```python
import onnx
onnx.save(model, "/path/to/model.onnx")
```

### モデルの整合性チェック

```python
import onnx
try:
    onnx.checker.check_model(model)
except onnx.checker.ValidationError as e:
    print(f"Invalid: {e}")
else:
    print("Valid")
```

### モデルの Shape 推論

```python
import onnx
from onnx import helper, shape_inference
inferred_model = shape_inference.infer_shapes(model) # 値に Shape が付与される
```

### モデルの Opset version 変換

```python
from onnx import version_converter, helper
model = onnx.load("/path/to/model.onnx")
# 古い opset version → 新しい opset version なら大抵成功する
new_model = version_converter.convert_version(original_model, TARGET_VERSION)
```

### モデルに含まれる Operator の種類の列挙

```python
import onnx
model = onnx.load("/path/to/model.onnx")
ops = set()
for node in model.graph.node:
    ops.add(node.op_type)
print(ops)
```

### モデルに含まれる値の列挙

```python
import onnx
model = onnx.load("../altius/models/mobilenetv3.onnx")
value_names = set()
# 入力
for input in model.graph.input:
    value_names.add(input.name)
# 出力
for output in model.graph.output:
    value_names.add(output.name)
# 初期値
for init in model.graph.initializer:
    value_names.add(init.name)
# Node の出力
for node in model.graph.node:
    for output in node.output:
        value_names.add(output)
print(value_names)
```

### モデルのグラフ構造を単純化

しばしば、モデルのグラフ構造が複雑（e.g., 使われてほしくない operator が含まれている）な場合があります。
その場合、[onnx-simplifier](https://github.com/daquexian/onnx-simplifier) を用いることで、単純化できる場合があります。

```python
import onnx
import onnxsim # pip install onnxsim
model = onnx.load("/path/to/model.onnx")
new_model, ok = onnxsim.simplify(model)
assert ok, "Failed to simplify"
```

### PyTorch モデルを ONNX としてエクスポート

[`torch.onnx.export`](https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export) を使うことが多いです。

```python
import torch
import torchvision
model = torchvision.model.mobilenet_v3_large(pretrained=True) # どんなモデルでも大抵は大丈夫
path = "/path/to/out_model.onnx"
torch.onnx.export(
    model,
    torch.randn(1, 3, 224, 224), # shape が正しければ値は関係ない
    path,
    opset_version=14 # 適切な opset version を自ら指定
)
```

### まだありそう

TODO...

## `ModelProto` の使いやすいラッパー

[`OnnxModel`](https://github.com/microsoft/onnxruntime/blob/9dd9461e655088240cc09d3541cbf7f5c6bb91f9/onnxruntime/python/tools/transformers/onnx_model.py#L32) のようなラッパーを用意することで、グラフの解析がしやすくなります。
いくつもスニペットを作成して、散らかっていくよりは良いかもしれません。

```python
# ...
model = onnx.load("/path/to/model.onnx")
new_model = OnnxModel(model)

# e.g., あるノードの children（そのノードの出力を使っているノード）を知りたい
children = new_model.get_children(specific_node)

# e.g., あるノードに定数入力があるならそれを知りたい
nth, const_node = new_model.get_constant_input(specific_node)
```
