---
title: "深層学習モデルの推論ランタイムを0から作った話"
emoji: "🧮"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["onnx", "rust", "深層学習"]
published: false
---

# はじめに

深層学習モデルを動作させるためのソフトウェアは数多くあります。
PyTorch や TensorFlow などのフレームワークはそれ自身がモデルを実行する機能を持っていますし、ONNX Runtime のようにモデルを動作させることに特化したソフトウェアも存在します。

これらのソフトウェアは大抵、Python などから簡単に扱うことができます。
しかしながら、それらがどのように動作しているのか疑問に思うことはないでしょうか。

この記事では、0 から深層学習モデルの推論ランタイム（長いので以下「深層学習ランタイム」）を作った過程で学んだことを、とりとめもなく紹介していきます。ほとんど、自分用のメモのようになってしまうかもしれません。
作ったものは以下のリポジトリにあります。
（~~技術的にはかなり適当なことを書いてしまうかもしれません。~~）

https://github.com/maekawatoshiki/altius


# 深層学習ランタイムは何をするのか

深層学習ランタイムは、深層学習モデルを動作させるためのソフトウェアです。
もう少し具体的に考えてみると、深層学習ランタイムは「モデル」と「入力データ」を受け取り、「出力データ」を返すソフトウェアだと言えます。（ここでは推論時のみを取り扱います。）

今は機械学習寄りの話をしているため、入力・出力データはテンソルとして表現されていることがほとんどです。
例えば、入力として shape が $1 \times 3 \times 224 \times 224$ の画像、出力として shape が $1 \times 1000$ のクラス分類、などが挙げられます。
（ここでは、テンソルは単に多次元配列だと捉えていただければ大丈夫です。流石に適当すぎるかもしれないですが。）

テンソルを受け取り、モデルの構造に沿って何らかの計算を行い、テンソルを返す。この一連の処理を行うのが深層学習ランタイムというわけですね。


# 深層学習モデルはどのように表現されるのか

深層学習モデルを動作させるためには、それがどのような構造をしているのか知る必要があります。
フレームワークやフォーマットごとに細かい部分は異なりますが、基本的に以下のような特徴を持ちます。

- 有向非巡回グラフ（DAG）
    - ノード: テンソルに対する操作（e.g. Add, Relu, Conv）
    - エッジ: テンソルの値（= データの流れを表現している）
- 入次数が 0 のノードに（入力）テンソルを渡すと、出次数が 0 のノードから（出力）テンソルが得られる
    - ノードは、テンソルを受け取り、それに対して何らかの操作（計算）を行い、テンソルを返す
    - あるノードからあるノードへと行き着くまでに、それぞれのノードの種類に対応する計算が行われる
    - 外から見ると、テンソルを与えたら、別のテンソルが得られるように見える
- DAG であるため、トポロジカルソートするとノードを一列に並べられる
    - 計算機で動かしやすい

また、広く使われているフォーマットとして、[ONNX](https://onnx.ai) が存在します。
ONNX 形式で表現されたモデルを可視化した例を以下に示します。（一番上の `Input3` はノードではなくて入力テンソルの名前、一番下の`Plus214_Output_0` は出力テンソルの名前、それ以外の四角形はノードです）

![mnist](https://cdn.thenewstack.io/media/2020/07/c601845f-onnx-mnist-0-328x1024.jpg)


# 苦労

ここまで色々と書いてきましたが、私が深層学習ランタイムを作り始めた頃はほとんど何も知らない状態でした。
どれくらい何も知らなかったのかというと、深層学習自体はもちろん、NumPy などのライブラリが配列（テンソル）をどのように扱っているのか・配列に対する操作にはどのようなものがあるのか、すらも知りませんでした。

例えば以下のようなことを知らなかったはずです。（思いつく限り列挙）

- N 次元配列がどう実現されるのか
    - 何次元の配列だろうが、（基本的に）値はメモリ上に一列に並んでいることは知っていた
        - `float A[H][W];` と定義された変数なら、`A[y][x]` と `A + (y * W + x)` のアドレスが同じことなどは理解していた
    - ただ、これを N 次元に拡張したとき、Stride という概念が登場してきて、最初は混乱した
        - `float A[N][C][H][W];` の Stride は例えば `[C*H*W, H*W, W, 1]` と表現できる
        - 便宜上、`S=[C*H*W, H*W, W, 1]` とおいて（`S[0]=C*H*W`, `S[3]=1`) 、
        - Stride を使えば、`A[x][y][z][w]` のアドレスが `A+x*S[0]+y*S[1]+z*S[2]+w*[3]` と表現できる（すごい！）
    - さらに、Layout といった概念も後から登場してきて、値がメモリ上に一列に並んでいることすらも幻想だったと気づいた
- Broadcast がどう実現されるのか
    - Broadcast という考え方を用いると、異なる shape のテンソル間の計算が可能になる
        - e.g. shape がそれぞれ `(8, 4)`, `(8, )` のテンソルを足して、`(8, 4)` のテンソルを得られる
        - なんとなく何が起きているのかは理解できたが、じゃあ実際計算機上でどう実現されているのかわからなかった
    - Broadcast は一部の Stride を 0 にすることで実現される、と知った時はとても賢いなぁと感激した覚えがある 
        - e.g. `float A[8][4];`, `float B[8];` の Stride は（このままだと）それぞれ `[4, 1]`, `[1]` になる
        - ここで、それぞれの Stride を `[4, 1]`, `[1, 0]` だと解釈してあげる
        - すると、要素和は `Output[y][x] = *(A+y*4+x*1) + *(B+y*1+x*0)` だと書ける（！）
- Reshape とは何なのか
    - shape を `(1, 3, 20, 20)` から `(1, 1200)` に変換できる、と言われてもピンとこなかった
    - `(1, 10)` から `(1, 2, 5)` のように次元が増える場合もある、と言われた時は頭が ??? で埋め尽くされた覚えがある
    - これは結局、Stride の値を書き換えているだけなんだと理解した
        - データの中身が変化することはない
- im2col とは何か
    - 畳み込み演算（Convolution）を素直に実装すると何重ものループを使うことになる
        - そして（少なくとも CPU 上なら）遅い
    - そこで画像を上手く変換してあげることで、重みとの行列積だけで Convolution を実現するのが im2col
        - ただしメモリを消費するので気を付ける
- 単純な行列積の実装は本当に遅い
    - 知識としては知っていたことだけれど、いざ手を動かしてみると実感する
    - OpenBLAS, Intel MKL, Blis は本当に速い
- Protocol Buffers の扱い方
    - これは ONNX に関係する話
    - Rust や Python で、自在に Protocol Buffers を扱う力がついた気がする
- Python
    - おそらく機械学習に入門していなければ、こんなに Python を書けるようにはならなかった気がする

さらに、以下のような理由から実装に苦労しました。

- 深層学習ランタイムを自作する人は少ない
    - 畳み込み演算を手書きする、MNIST の推論モデルを手書きする、くらいならいくつか見つけることができました
    - もっと調べれば見つかるのかもしれませんが、既存のランタイムのようにさまざまなモデルを動作させている人がなかなか見つかりませんでした
    - 有名で大きなソフトウェアを参考にするしかない
        - Rust 製で参考になったもの:
            - https://github.com/sonos/tract （ちょっと大きい）
            - https://github.com/webonnx/wonnx （比較的小さめ）
        - あとは PyTorch や ONNXRuntime, TVM, Glow などを読んでいました
